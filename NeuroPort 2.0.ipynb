{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import compress\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from scipy import interpolate\n",
    "import math\n",
    "import optuna\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GroupKFold, LeaveOneGroupOut, GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from sklearn.utils import resample\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer, make_column_selector\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from mne.decoding import UnsupervisedSpatialFilter, SlidingEstimator\n",
    "from mne.preprocessing.nirs import optical_density, beer_lambert_law, scalp_coupling_index, temporal_derivative_distribution_repair\n",
    "from mne_nirs.channels import get_long_channels, get_short_channels\n",
    "from mne_nirs.channels import picks_pair_to_idx as p2idx\n",
    "from mne_nirs.signal_enhancement import (enhance_negative_correlation,\n",
    "                                         short_channel_regression)\n",
    "from mne.viz import plot_compare_evokeds\n",
    "from mne_nirs.experimental_design import make_first_level_design_matrix\n",
    "from nilearn.plotting import plot_design_matrix\n",
    "\n",
    "import mne\n",
    "from mne.decoding import Scaler, cross_val_multiscore, Vectorizer\n",
    "from mne import Epochs, events_from_annotations, set_log_level\n",
    "from mne.io import read_raw_snirf\n",
    "from mne_nirs.io.snirf import write_raw_snirf\n",
    "from mne_bids import write_raw_bids, BIDSPath, read_raw_bids\n",
    "\n",
    "# Feature extraction\n",
    "from tsfresh import extract_features, extract_relevant_features, select_features, feature_extraction\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "from tsfresh.feature_extraction.feature_calculators import set_property\n",
    "from tsfresh.feature_extraction import feature_calculators\n",
    "\n",
    "# Feature importance\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "import h5py as h "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def individual_analysis(bids_path):\n",
    "    # Read data with annotations in BIDS format\n",
    "    raw_intensity = read_raw_bids(bids_path, verbose=False)\n",
    "\n",
    "    # Convert signal to optical density and determine bad channels\n",
    "    raw_od = optical_density(raw_intensity)\n",
    "    sci = scalp_coupling_index(raw_od, h_freq=1.35, h_trans_bandwidth=0.1)\n",
    "    raw_od.info[\"bads\"] = list(compress(raw_od.ch_names, sci < 0.5))\n",
    "    raw_od.interpolate_bads()\n",
    "\n",
    "    # Downsample and apply signal cleaning techniques\n",
    "    raw_od.resample(1.0)\n",
    "    raw_od = temporal_derivative_distribution_repair(raw_od)\n",
    "    raw_od = short_channel_regression(raw_od)\n",
    "\n",
    "    # Convert to haemoglobin and filter\n",
    "    raw_haemo = beer_lambert_law(raw_od)\n",
    "    \n",
    "    ## FILTERS OUT HEART RATE\n",
    "    raw_haemo = raw_haemo.filter(None, 0.4,\n",
    "                                 h_trans_bandwidth=0.1, l_trans_bandwidth=0.01,\n",
    "                                 verbose=False)\n",
    "    raw_haemo.annotations.delete(raw_haemo.annotations.description == '15')\n",
    "    \n",
    "    # Apply further data cleaning techniques and extract epochs\n",
    "    raw_haemo = enhance_negative_correlation(raw_haemo)\n",
    "\n",
    "    # raw_haemo = get_long_channels(raw_haemo, min_dist=0.01)\n",
    "\n",
    "    # Pick data channels that are actually informative\n",
    "    # roi_channels = mne.pick_channels(raw_haemo.info['ch_names'], include=['Left_PT','Right_PT'])\n",
    "    # raw_haemo = raw_haemo.copy().pick_channels(roi_channels)\n",
    "\n",
    "    # Extract events but ignore those with\n",
    "    events, event_dict = events_from_annotations(raw_haemo, verbose=False,\n",
    "                                                 regexp='^(?![Ends]).*$')\n",
    "    \n",
    "    epochs = Epochs(raw_haemo, events, event_id=event_dict, tmin=-5, tmax=30,\n",
    "                    reject=dict(hbo=100e-6), reject_by_annotation=True,\n",
    "                    proj=True, baseline=(None, 0), detrend=1,\n",
    "                    preload=True, verbose=False,event_repeated='merge')\n",
    "\n",
    "    return raw_haemo, epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading C:\\Users\\dalto\\Downloads\\project\\sourcedata_stim\\sub-01\\ses-01\\nirs\\sub-01_ses-01_nirs.snirf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dalto\\AppData\\Local\\Temp\\ipykernel_20036\\1699478881.py:45: RuntimeWarning: Did not find any events.tsv associated with sub-01_ses-01.\n",
      "\n",
      "The search_str was \"C:\\Users\\dalto\\Downloads\\project\\sourcedata_stim\\sub-01\\**\\nirs\\sub-01_ses-01*events.tsv\"\n",
      "  raw_haemo = read_raw_bids(raw_path)\n",
      "C:\\Users\\dalto\\AppData\\Local\\Temp\\ipykernel_20036\\1699478881.py:45: RuntimeWarning: Did not find any channels.tsv associated with sub-01_ses-01.\n",
      "\n",
      "The search_str was \"C:\\Users\\dalto\\Downloads\\project\\sourcedata_stim\\sub-01\\**\\nirs\\sub-01_ses-01*channels.tsv\"\n",
      "  raw_haemo = read_raw_bids(raw_path)\n",
      "C:\\Users\\dalto\\AppData\\Local\\Temp\\ipykernel_20036\\1699478881.py:45: RuntimeWarning: Did not find any nirs.json associated with sub-01_ses-01.\n",
      "\n",
      "The search_str was \"C:\\Users\\dalto\\Downloads\\project\\sourcedata_stim\\sub-01\\**\\nirs\\sub-01_ses-01*nirs.json\"\n",
      "  raw_haemo = read_raw_bids(raw_path)\n",
      "C:\\Users\\dalto\\AppData\\Local\\Temp\\ipykernel_20036\\1699478881.py:45: RuntimeWarning: participants.tsv file not found for C:\\Users\\dalto\\Downloads\\project\\sourcedata_stim\\sub-01\\ses-01\\nirs\\sub-01_ses-01_nirs.snirf\n",
      "  raw_haemo = read_raw_bids(raw_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw.info: 43\n",
      "\n",
      "x available lists: ['dataTimeSeries', 'measurementList1', 'measurementList10', 'measurementList11', 'measurementList12', 'measurementList13', 'measurementList14', 'measurementList15', 'measurementList16', 'measurementList17', 'measurementList18', 'measurementList19', 'measurementList2', 'measurementList20', 'measurementList21', 'measurementList22', 'measurementList23', 'measurementList24', 'measurementList25', 'measurementList26', 'measurementList27', 'measurementList28', 'measurementList29', 'measurementList3', 'measurementList30', 'measurementList31', 'measurementList32', 'measurementList33', 'measurementList34', 'measurementList35', 'measurementList36', 'measurementList37', 'measurementList38', 'measurementList39', 'measurementList4', 'measurementList40', 'measurementList41', 'measurementList42', 'measurementList43', 'measurementList44', 'measurementList45', 'measurementList46', 'measurementList47', 'measurementList48', 'measurementList49', 'measurementList5', 'measurementList50', 'measurementList51', 'measurementList52', 'measurementList53', 'measurementList54', 'measurementList55', 'measurementList56', 'measurementList57', 'measurementList58', 'measurementList59', 'measurementList6', 'measurementList60', 'measurementList61', 'measurementList62', 'measurementList63', 'measurementList64', 'measurementList65', 'measurementList66', 'measurementList67', 'measurementList68', 'measurementList69', 'measurementList7', 'measurementList70', 'measurementList71', 'measurementList72', 'measurementList73', 'measurementList74', 'measurementList75', 'measurementList76', 'measurementList77', 'measurementList78', 'measurementList79', 'measurementList8', 'measurementList80', 'measurementList81', 'measurementList82', 'measurementList83', 'measurementList84', 'measurementList85', 'measurementList86', 'measurementList9', 'time']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dalto\\AppData\\Local\\Temp\\ipykernel_20036\\1699478881.py:111: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  target['id'] = target['id'].astype(int)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>id</th>\n",
       "      <th>is_event</th>\n",
       "      <th>S1_D1 760</th>\n",
       "      <th>S1_D16 760</th>\n",
       "      <th>S2_D3 760</th>\n",
       "      <th>S3_D10 760</th>\n",
       "      <th>S4_D3 760</th>\n",
       "      <th>S4_D17 760</th>\n",
       "      <th>S5_D5 760</th>\n",
       "      <th>...</th>\n",
       "      <th>S9_D8 850</th>\n",
       "      <th>S9_D20 850</th>\n",
       "      <th>S10_D10 850</th>\n",
       "      <th>S11_D11 850</th>\n",
       "      <th>S12_D12 850</th>\n",
       "      <th>S13_D12 850</th>\n",
       "      <th>S13_D14 850</th>\n",
       "      <th>S13_D22 850</th>\n",
       "      <th>S14_D15 850</th>\n",
       "      <th>S15_D15 850</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>event_index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50.50368</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005579</td>\n",
       "      <td>0.326369</td>\n",
       "      <td>0.014329</td>\n",
       "      <td>0.209916</td>\n",
       "      <td>0.001204</td>\n",
       "      <td>0.251827</td>\n",
       "      <td>0.110648</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007882</td>\n",
       "      <td>0.641227</td>\n",
       "      <td>0.077501</td>\n",
       "      <td>0.029581</td>\n",
       "      <td>0.057954</td>\n",
       "      <td>0.051245</td>\n",
       "      <td>0.012981</td>\n",
       "      <td>0.600144</td>\n",
       "      <td>0.056344</td>\n",
       "      <td>0.211949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>82.20672</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005399</td>\n",
       "      <td>0.325262</td>\n",
       "      <td>0.013784</td>\n",
       "      <td>0.208943</td>\n",
       "      <td>0.001195</td>\n",
       "      <td>0.252196</td>\n",
       "      <td>0.107675</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007863</td>\n",
       "      <td>0.638065</td>\n",
       "      <td>0.076755</td>\n",
       "      <td>0.030328</td>\n",
       "      <td>0.059199</td>\n",
       "      <td>0.051477</td>\n",
       "      <td>0.013225</td>\n",
       "      <td>0.596289</td>\n",
       "      <td>0.056192</td>\n",
       "      <td>0.214306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>108.93312</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005575</td>\n",
       "      <td>0.327725</td>\n",
       "      <td>0.014381</td>\n",
       "      <td>0.212541</td>\n",
       "      <td>0.001231</td>\n",
       "      <td>0.250746</td>\n",
       "      <td>0.106188</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007956</td>\n",
       "      <td>0.642397</td>\n",
       "      <td>0.077172</td>\n",
       "      <td>0.030271</td>\n",
       "      <td>0.059657</td>\n",
       "      <td>0.052366</td>\n",
       "      <td>0.013334</td>\n",
       "      <td>0.592969</td>\n",
       "      <td>0.058841</td>\n",
       "      <td>0.219273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>141.00480</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005452</td>\n",
       "      <td>0.324756</td>\n",
       "      <td>0.013870</td>\n",
       "      <td>0.208370</td>\n",
       "      <td>0.001197</td>\n",
       "      <td>0.250562</td>\n",
       "      <td>0.102312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007755</td>\n",
       "      <td>0.632151</td>\n",
       "      <td>0.075157</td>\n",
       "      <td>0.029914</td>\n",
       "      <td>0.058589</td>\n",
       "      <td>0.051375</td>\n",
       "      <td>0.013085</td>\n",
       "      <td>0.585626</td>\n",
       "      <td>0.056185</td>\n",
       "      <td>0.212025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>184.32000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005458</td>\n",
       "      <td>0.324324</td>\n",
       "      <td>0.014248</td>\n",
       "      <td>0.208847</td>\n",
       "      <td>0.001245</td>\n",
       "      <td>0.251236</td>\n",
       "      <td>0.109453</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007643</td>\n",
       "      <td>0.633200</td>\n",
       "      <td>0.075513</td>\n",
       "      <td>0.029983</td>\n",
       "      <td>0.060188</td>\n",
       "      <td>0.052528</td>\n",
       "      <td>0.013348</td>\n",
       "      <td>0.587346</td>\n",
       "      <td>0.058945</td>\n",
       "      <td>0.212377</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  time  id  is_event  S1_D1 760  S1_D16 760  S2_D3 760  \\\n",
       "event_index                                                              \n",
       "1             50.50368   0         0   0.005579    0.326369   0.014329   \n",
       "2             82.20672   0         0   0.005399    0.325262   0.013784   \n",
       "3            108.93312   1         0   0.005575    0.327725   0.014381   \n",
       "4            141.00480   1         0   0.005452    0.324756   0.013870   \n",
       "5            184.32000   0         0   0.005458    0.324324   0.014248   \n",
       "\n",
       "             S3_D10 760  S4_D3 760  S4_D17 760  S5_D5 760  ...  S9_D8 850  \\\n",
       "event_index                                                ...              \n",
       "1              0.209916   0.001204    0.251827   0.110648  ...   0.007882   \n",
       "2              0.208943   0.001195    0.252196   0.107675  ...   0.007863   \n",
       "3              0.212541   0.001231    0.250746   0.106188  ...   0.007956   \n",
       "4              0.208370   0.001197    0.250562   0.102312  ...   0.007755   \n",
       "5              0.208847   0.001245    0.251236   0.109453  ...   0.007643   \n",
       "\n",
       "             S9_D20 850  S10_D10 850  S11_D11 850  S12_D12 850  S13_D12 850  \\\n",
       "event_index                                                                   \n",
       "1              0.641227     0.077501     0.029581     0.057954     0.051245   \n",
       "2              0.638065     0.076755     0.030328     0.059199     0.051477   \n",
       "3              0.642397     0.077172     0.030271     0.059657     0.052366   \n",
       "4              0.632151     0.075157     0.029914     0.058589     0.051375   \n",
       "5              0.633200     0.075513     0.029983     0.060188     0.052528   \n",
       "\n",
       "             S13_D14 850  S13_D22 850  S14_D15 850  S15_D15 850  \n",
       "event_index                                                      \n",
       "1               0.012981     0.600144     0.056344     0.211949  \n",
       "2               0.013225     0.596289     0.056192     0.214306  \n",
       "3               0.013334     0.592969     0.058841     0.219273  \n",
       "4               0.013085     0.585626     0.056185     0.212025  \n",
       "5               0.013348     0.587346     0.058945     0.212377  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "hbo_by_event shape: (24, 46)\n",
      "target_all shape: (8166,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_id(time):\n",
    "    time_in_stims = stims['time']\n",
    "    matches = stims[(time_in_stims >= time - 30) & (time_in_stims <= time + 5)]\n",
    "    if len(matches) > 0:\n",
    "        # print(f\"\\nmatches: \\n{matches}\\n\")\n",
    "        return matches.iloc[0]['id']\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# for ch_type in [\"fnirs\", \"hbo\", \"hbr\"]:\n",
    "for ch_type in [\"hbo\"]:\n",
    "    x_all = []\n",
    "    y_all = []\n",
    "    groups = []\n",
    "\n",
    "    for sub in range(1,2):\n",
    "        # hbo = pd.DataFrame()\n",
    "        # target = pd.Series()\n",
    "        \n",
    "        subject_id = \"%02d\" % sub\n",
    "        raw_path = BIDSPath(\n",
    "            subject=\"%02d\" % sub,\n",
    "            # task=\"task\",\n",
    "            session='01',\n",
    "            datatype=\"nirs\",\n",
    "            # suffix='nirs',\n",
    "            root=r\"C:\\Users\\dalto\\Downloads\\project\\sourcedata_stim\",\n",
    "            extension=\".snirf\"\n",
    "        )\n",
    "\n",
    "        # events, event_id = mne.events_from_annotations(mne.io.read_raw_snirf(raw_path))\n",
    "        # raw_haemo, epochs = individual_analysis(raw_path)\n",
    "\n",
    "        # print(f\"epochs: \\n{epochs.events[37:, [0, 2]]}\\n\")\n",
    "        # print(f\"epochs size: {raw_haemo.info}\\n\")\n",
    "\n",
    "        raw_haemo = read_raw_bids(raw_path)\n",
    "\n",
    "        hbo_cols = raw_haemo.info.ch_names[::2]\n",
    "        print(f\"raw.info: {len(hbo_cols)}\\n\")\n",
    "        \n",
    "\n",
    "        with h.File(raw_path,'r') as f:\n",
    "            haemo_multi = f['nirs/data1']\n",
    "            x_list = list(haemo_multi)\n",
    "            print(f\"x available lists: {x_list}\\n\")\n",
    "\n",
    "            x_data = haemo_multi.get('dataTimeSeries')\n",
    "            x_time = np.array(haemo_multi.get('time'))\n",
    "\n",
    "            hbo_dataset = np.array(x_data[:, ::2])\n",
    "\n",
    "            stim1 = np.array(f['nirs/stim1/data'])[:,0]\n",
    "            stim3 = np.array(f['nirs/stim2/data'])[:,0]\n",
    "\n",
    "\n",
    "        hbo = pd.DataFrame(hbo_dataset)\n",
    "        stim1 = pd.DataFrame(stim1)\n",
    "        stim3 = pd.DataFrame(stim3)\n",
    "\n",
    "        # sets hbo dataFrame column names to match their channel and adds a column for time\n",
    "        hbo.rename(columns={old_col: new_col for old_col, new_col in zip(hbo.columns, hbo_cols)}, inplace=True)\n",
    "        hbo.insert(0, 'time', x_time)\n",
    "\n",
    "        # creates a single dataframe to hold all stimulus values\n",
    "        stim1[\"id\"] = 1\n",
    "        stim3[\"id\"] = 0\n",
    "        stim1.rename(columns={stim1.columns[0]: \"time\"}, inplace=True)\n",
    "        stim3.rename(columns={stim3.columns[0]: \"time\"}, inplace=True)\n",
    "        stims = pd.concat([stim1, stim3]).sort_values(by='time')\n",
    "\n",
    "        # drops all non event related data\n",
    "        hbo['id'] = hbo['time'].apply(get_id)\n",
    "        hbo.insert(1, 'id', hbo.pop('id'))\n",
    "        hbo.dropna(inplace=True)\n",
    "        \n",
    "        # sets is_event column to contain whether or not this is the exact time that stimulus was provided\n",
    "        hbo['is_event'] = 0\n",
    "        for stim_time in stims['time']:\n",
    "            min_diff = np.abs(hbo['time'] - stim_time).idxmin()\n",
    "            hbo.loc[min_diff, 'is_event'] = 1\n",
    "        hbo.insert(2, 'is_event', hbo.pop('is_event'))\n",
    "\n",
    "        target = stims\n",
    "\n",
    "        event_indices = hbo[hbo['is_event'] == 1].index\n",
    "        hbo.insert(2, 'event_index', 0)\n",
    "\n",
    "        # Set the 'event_index' value to 1 for the rows 5 seconds before 'is_event' equals 1\n",
    "        for index in event_indices:\n",
    "            time_of_event = hbo.loc[index, 'time']\n",
    "            time_before = time_of_event - 5\n",
    "            if hbo.empty:\n",
    "                hbo.loc[hbo['time'] >= time_before, 'event_index'] += 1\n",
    "                # print('should not have ran')\n",
    "            else:\n",
    "                hbo.loc[hbo['time'] >= time_before, 'event_index'] += 1\n",
    "                # hbo.loc[hbo['time'] >= time_before, 'event_index'] += 1\n",
    "\n",
    "        target = hbo[[\"event_index\",\"id\"]]\n",
    "        \n",
    "        hbo['id'] = hbo['id'].astype(int)\n",
    "        target['id'] = target['id'].astype(int)\n",
    "\n",
    "        hbo_by_event = hbo.copy()\n",
    "        # target.drop_duplicates(subset=['event_index'], inplace=True)    \n",
    "        hbo_by_event.drop_duplicates(subset=['event_index'], inplace=True)\n",
    "\n",
    "        target = target.set_index('event_index')\n",
    "        # hbo = hbo.set_index('event_index')\n",
    "        hbo_by_event = hbo_by_event.set_index('event_index')\n",
    "\n",
    "\n",
    "        target = pd.Series(data=target[\"id\"], index=target.index)\n",
    "        hbo = hbo.drop(columns = ['id','time'])\n",
    "        \n",
    "\n",
    "        # display(target.head())\n",
    "        # print(f\"\\ntarget shape: {target.shape}\")\n",
    "\n",
    "        # display(hbo.tail())\n",
    "        # print(f\"\\nhbo shape: {hbo.shape}\")\n",
    "\n",
    "        display(hbo_by_event.head())\n",
    "        print(f\"\\nhbo_by_event shape: {hbo_by_event.shape}\")\n",
    "\n",
    "        hbo = pd.concat([hbo,hbo], ignore_index=True)\n",
    "        target_all = pd.concat([target,target],ignore_index=True)\n",
    "        \n",
    "\n",
    "        # display(hbo.tail())\n",
    "        # print(f\"hbo shape: {hbo.shape}\")\n",
    "\n",
    "        # display(target_all.head())\n",
    "        # print(f\"target_all shape: {target_all.shape}\")\n",
    "\n",
    "# target_all = pd.Series(data=target_all[\"id\"], index=target_all.index)\n",
    "print(f\"target_all shape: {target_all.shape}\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading C:\\Users\\dalto\\Downloads\\project\\sourcedata_lm\\sub-01\\nirs\\sub-01_task-task_nirs.snirf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading events from C:\\Users\\dalto\\Downloads\\project\\sourcedata_lm\\sub-01\\nirs\\sub-01_task-task_events.tsv.\n",
      "Reading channel info from C:\\Users\\dalto\\Downloads\\project\\sourcedata_lm\\sub-01\\nirs\\sub-01_task-task_channels.tsv.\n",
      "Not fully anonymizing info - keeping his_id, sex, and hand info\n",
      "raw.info: 46\n",
      "\n",
      "x available lists: ['dataTimeSeries', 'measurementList1', 'measurementList10', 'measurementList11', 'measurementList12', 'measurementList13', 'measurementList14', 'measurementList15', 'measurementList16', 'measurementList17', 'measurementList18', 'measurementList19', 'measurementList2', 'measurementList20', 'measurementList21', 'measurementList22', 'measurementList23', 'measurementList24', 'measurementList25', 'measurementList26', 'measurementList27', 'measurementList28', 'measurementList29', 'measurementList3', 'measurementList30', 'measurementList31', 'measurementList32', 'measurementList33', 'measurementList34', 'measurementList35', 'measurementList36', 'measurementList37', 'measurementList38', 'measurementList39', 'measurementList4', 'measurementList40', 'measurementList41', 'measurementList42', 'measurementList43', 'measurementList44', 'measurementList45', 'measurementList46', 'measurementList47', 'measurementList48', 'measurementList49', 'measurementList5', 'measurementList50', 'measurementList51', 'measurementList52', 'measurementList53', 'measurementList54', 'measurementList55', 'measurementList56', 'measurementList57', 'measurementList58', 'measurementList59', 'measurementList6', 'measurementList60', 'measurementList61', 'measurementList62', 'measurementList63', 'measurementList64', 'measurementList65', 'measurementList66', 'measurementList67', 'measurementList68', 'measurementList69', 'measurementList7', 'measurementList70', 'measurementList71', 'measurementList72', 'measurementList73', 'measurementList74', 'measurementList75', 'measurementList76', 'measurementList77', 'measurementList78', 'measurementList79', 'measurementList8', 'measurementList80', 'measurementList81', 'measurementList82', 'measurementList83', 'measurementList84', 'measurementList85', 'measurementList86', 'measurementList87', 'measurementList88', 'measurementList89', 'measurementList9', 'measurementList90', 'measurementList91', 'measurementList92', 'time']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sub in range(1,2):\n",
    "        # hbo = pd.DataFrame()\n",
    "        # target = pd.Series()\n",
    "        \n",
    "        subject_id = \"%02d\" % sub\n",
    "        raw_path = BIDSPath(\n",
    "            subject=\"%02d\" % sub,\n",
    "            task=\"task\",\n",
    "            # session='01',\n",
    "            datatype=\"nirs\",\n",
    "            suffix='nirs',\n",
    "            root=r\"C:\\Users\\dalto\\Downloads\\project\\sourcedata_lm\",\n",
    "            extension=\".snirf\"\n",
    "        )\n",
    "\n",
    "        # events, event_id = mne.events_from_annotations(mne.io.read_raw_snirf(raw_path))\n",
    "        # raw_haemo, epochs = individual_analysis(raw_path)\n",
    "\n",
    "        # print(f\"epochs: \\n{epochs.events[37:, [0, 2]]}\\n\")\n",
    "        # print(f\"epochs size: {raw_haemo.info}\\n\")\n",
    "\n",
    "        raw_haemo = read_raw_bids(raw_path)\n",
    "\n",
    "        hbo_cols = raw_haemo.info.ch_names[::2]\n",
    "        print(f\"raw.info: {len(hbo_cols)}\\n\")\n",
    "        \n",
    "\n",
    "        with h.File(raw_path,'r') as f:\n",
    "            haemo_multi = f['nirs/data1']\n",
    "            x_list = list(haemo_multi)\n",
    "            print(f\"x available lists: {x_list}\\n\")\n",
    "\n",
    "            x_data = haemo_multi.get('dataTimeSeries')\n",
    "            x_time = np.array(haemo_multi.get('time'))\n",
    "            # print(f\"x time: \\n{x_time}\\n\")\n",
    "\n",
    "            hbo_dataset = np.array(x_data[:, ::2])\n",
    "\n",
    "            stim1 = np.array(f['nirs/stim1/data'])[:,0]\n",
    "            stim3 = np.array(f['nirs/stim3/data'])[:,0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Exrtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@set_property(\"fctype\", \"simple\")\n",
    "\n",
    "def evoked(x):\n",
    "    \"\"\"\n",
    "    Distance traveled in an axis\n",
    "\n",
    "    :param x: the time series to calculate the feature of\n",
    "    :type x: pandas.Series\n",
    "    :return: the value of this feature\n",
    "    :return type: bool, int or float\n",
    "    \"\"\"\n",
    "    # Distance traveled in one axis\n",
    "    result = x[-1] - x[0]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_parameters = {\n",
    "                 'abs_energy':None, # Returns the absolute energy of the time series which is the sum over the squared values\n",
    "                 'absolute_sum_of_changes':None, # Returns the sum over the absolute value of consecutive changes in the series x\n",
    "                 'fft_aggregated':[ {'aggtype': 'centroid'}, # Returns the spectral centroid (mean), \n",
    "                                    {'aggtype': 'variance'}, # variance, \n",
    "                                    {'aggtype': 'skew'},     # skew, \n",
    "                                    {'aggtype': 'kurtosis'}], # and kurtosis of the absolute fourier transform spectrum.\n",
    "                 'c3': [{'lag': 1}, {'lag': 2}, {'lag': 3}], # Uses c3 statistics to measure non linearity in the time series\n",
    "                 'standard_deviation': None, # Returns the standard deviation of x\n",
    "                 'variance': None, # Returns the variance of x\n",
    "                 'skewness': None, # Returns the sample skewness of x (calculated with the adjusted Fisher-Pearson standardized moment coefficient G1).\n",
    "                 'kurtosis': None, # Returns the kurtosis of x (calculated with the adjusted Fisher-Pearson standardized moment coefficient G2).\n",
    "                 'maximum': None, # Calculates the highest value of the time series x.\n",
    "                 'minimum': None, # Calculates the lowest value of the time series x.\n",
    "                 'sample_entropy':None, # Calculate and return sample entropy of x.\n",
    "                 'mean_abs_change':None, # Average over first differences.\n",
    "                 'sum_values':None, # Calculates the sum over the time series values\n",
    "                #  'evoked': None # Calculates the evoked amplitude of the waveform\n",
    "                # 'id' : 'id'\n",
    "                }\n",
    "\n",
    "feature_calculators.__dict__[\"evoked\"] = evoked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Features Using tsfresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = extract_features(hbo, \n",
    "#                      column_id=\"event_index\", \n",
    "#                      column_sort=\"time\", \n",
    "#                      default_fc_parameters=fc_parameters, \n",
    "#                      impute_function=impute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"\\nx.shape: {x.shape}\")\n",
    "# x.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object split_data at 0x000001FEC84414D0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x_train, x_test, y_train, y_test = train_test_split(x, target, test_size=0.2, random_state=1)\n",
    "\n",
    "random_state_list = [42]\n",
    "n_splits = 10\n",
    "\n",
    "\n",
    "def split_data(X, y, random_state_list):\n",
    "        for random_state in random_state_list:\n",
    "            kf = StratifiedKFold(n_splits=n_splits, random_state=random_state, shuffle=True)\n",
    "            for train_index, val_index in kf.split(X, y):\n",
    "                X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "                y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "                yield X_train, X_val, y_train, y_val\n",
    "\n",
    "split_data(hbo,target_all,random_state_list)\n",
    "\n",
    "# x_train = x_train.sort_index()\n",
    "\n",
    "# x_train['id'] = hbo_by_event['id']\n",
    "# x_test['id'] = hbo_by_event['id']\n",
    "\n",
    "# x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train = y_train.sort_index()\n",
    "\n",
    "# y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_filtered_direct = select_features(x_train, \n",
    "#                                            y_train,\n",
    "#                                            ml_task=\"classification\", # Classification o regression, by default set to auto\n",
    "#                                            fdr_level = 0 # Respected percentage of features to be discarded as irrelevant, by default set to 0.05\n",
    "#                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_filtered_direct.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DTC = DecisionTreeClassifier()\n",
    "# DTC.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m KNN \u001b[39m=\u001b[39m KNeighborsClassifier(n_neighbors\u001b[39m=\u001b[39m\u001b[39m21\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m KNN\u001b[39m.\u001b[39mfit(x_train,y_train)\n\u001b[0;32m      5\u001b[0m KNN\u001b[39m.\u001b[39mscore(x_test,y_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "# KNN = KNeighborsClassifier(n_neighbors=21)\n",
    "\n",
    "# KNN.fit(x_train,y_train)\n",
    "\n",
    "# KNN.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9706242350061199\n",
      "0.9596083231334149\n",
      "0.9681762545899633\n",
      "0.9632802937576499\n",
      "0.9681762545899633\n",
      "0.9596083231334149\n",
      "0.9681372549019608\n",
      "0.9705882352941176\n",
      "0.9607843137254902\n",
      "0.9705882352941176\n",
      "\n",
      "average scores: 0.9659571723426212\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "for i, (xtrain, x_val, ytrain, y_val) in enumerate(split_data(hbo, target_all, random_state_list=random_state_list)):\n",
    "    n = i % n_splits\n",
    "    m = i // n_splits\n",
    "    model = KNeighborsClassifier(n_neighbors=50)\n",
    "    model.fit(xtrain, ytrain)\n",
    "    score = model.score(x_val, y_val)\n",
    "    scores.append(score)\n",
    "    print(score)\n",
    "\n",
    "print(f\"\\naverage scores: {np.average(scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.631578947368421\n",
      "0.6144430844553244\n",
      "0.5936352509179926\n",
      "0.6119951040391677\n",
      "0.6107711138310894\n",
      "0.602203182374541\n",
      "0.6115196078431373\n",
      "0.6066176470588235\n",
      "0.625\n",
      "0.6053921568627451\n",
      "\n",
      "average scores: 0.6113156094751242\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "for i, (xtrain, x_val, ytrain, y_val) in enumerate(split_data(hbo, target_all, random_state_list=random_state_list)):\n",
    "    n = i % n_splits\n",
    "    m = i // n_splits\n",
    "    model = SVC()\n",
    "    model.fit(xtrain, ytrain)\n",
    "    score = model.score(x_val, y_val)\n",
    "    scores.append(score)\n",
    "    print(score)\n",
    "\n",
    "print(f\"\\naverage scores: {np.average(scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.9975520195838433\n",
      "0.9951040391676866\n",
      "0.9975520195838433\n",
      "1.0\n",
      "1.0\n",
      "0.9975490196078431\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "\n",
      "average scores: 0.9987757097943216\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "for i, (xtrain, x_val, ytrain, y_val) in enumerate(split_data(hbo, target_all, random_state_list=random_state_list)):\n",
    "    n = i % n_splits\n",
    "    m = i // n_splits\n",
    "    model = xgb.XGBClassifier()\n",
    "    model.fit(xtrain, ytrain)\n",
    "    score = model.score(x_val, y_val)\n",
    "    scores.append(score)\n",
    "    print(score)\n",
    "\n",
    "print(f\"\\naverage scores: {np.average(scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m((y_test, KNN\u001b[39m.\u001b[39mpredict(x_test)))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "print((y_test, KNN.predict(x_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
